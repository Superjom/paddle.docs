

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>单双层RNN API对比介绍 &mdash; PaddlePaddle  文档</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  

  
  
        <link rel="index" title="索引"
              href="../../../genindex.html"/>
        <link rel="search" title="搜索" href="../../../search.html"/>
    <link rel="top" title="PaddlePaddle  文档" href="../../../index.html"/>
        <link rel="up" title="RNN相关模型" href="index_cn.html"/>
        <link rel="next" title="GPU性能分析与调优" href="../../optimization/gpu_profiling_cn.html"/>
        <link rel="prev" title="支持双层序列作为输入的Layer" href="hierarchical_layer_cn.html"/> 

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/perfect-scrollbar/0.6.14/css/perfect-scrollbar.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/override.css" type="text/css" />
  <script>
  var _hmt = _hmt || [];
  (function() {
    var hm = document.createElement("script");
    hm.src = "//hm.baidu.com/hm.js?b9a314ab40d04d805655aab1deee08ba";
    var s = document.getElementsByTagName("script")[0]; 
    s.parentNode.insertBefore(hm, s);
  })();
  </script>

  

  
  <script src="../../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  
  <header class="site-header">
    <div class="site-logo">
      <a href="/"><img src="../../../_static/images/PP_w.png"></a>
    </div>
    <div class="site-nav-links">
      <div class="site-menu">
        <a class="fork-on-github" href="https://github.com/PaddlePaddle/Paddle" target="_blank"><i class="fa fa-github"></i>Folk me on Github</a>
        <div class="language-switcher dropdown">
          <a type="button" data-toggle="dropdown">
            <span>English</span>
            <i class="fa fa-angle-up"></i>
            <i class="fa fa-angle-down"></i>
          </a>
          <ul class="dropdown-menu">
            <li><a href="/doc_cn">中文</a></li>
            <li><a href="/doc">English</a></li>
          </ul>
        </div>
        <ul class="site-page-links">
          <li><a href="/">Home</a></li>
        </ul>
      </div>
      <div class="doc-module">
        
        <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../getstarted/index_cn.html">新手入门</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../index_cn.html">进阶指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/index_cn.html">API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/index_cn.html">FAQ</a></li>
</ul>

        
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>        
      </div>
    </div>
  </header>
  
  <div class="main-content-wrap">

    
    <nav class="doc-menu-vertical" role="navigation">
        
          
          <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../getstarted/index_cn.html">新手入门</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../getstarted/build_and_install/index_cn.html">安装与编译</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../getstarted/build_and_install/docker_install_cn.html">PaddlePaddle的Docker容器使用方式</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../getstarted/build_and_install/ubuntu_install_cn.html">Ubuntu部署PaddlePaddle</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../getstarted/build_and_install/cmake/build_from_source_cn.html">PaddlePaddle的编译选项</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../getstarted/concepts/use_concepts_cn.html">基本使用概念</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../../index_cn.html">进阶指南</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../usage/cmd_parameter/index_cn.html">设置命令行参数</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../usage/cmd_parameter/use_case_cn.html">使用案例</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../usage/cmd_parameter/arguments_cn.html">参数概述</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../usage/cmd_parameter/detail_introduction_cn.html">细节描述</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../usage/cluster/cluster_train_cn.html">运行分布式训练</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../usage/k8s/k8s_basis_cn.html">Kubernetes 简介</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../usage/k8s/k8s_cn.html">Kubernetes单机训练</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../usage/k8s/k8s_distributed_cn.html">Kubernetes分布式训练</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../dev/write_docs_cn.html">如何贡献/修改文档</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../dev/contribute_to_paddle_cn.html">如何贡献代码</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="index_cn.html">RNN相关模型</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="recurrent_group_cn.html">Recurrent Group教程</a></li>
<li class="toctree-l3"><a class="reference internal" href="hierarchical_layer_cn.html">支持双层序列作为输入的Layer</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">单双层RNN API对比介绍</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../optimization/gpu_profiling_cn.html">GPU性能分析与调优</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/index_cn.html">API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/v2/model_configs.html">模型配置</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../api/v2/config/activation.html">Activation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/v2/config/layer.html">Layers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/v2/config/optimizer.html">Optimizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/v2/config/pooling.html">Pooling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/v2/config/networks.html">Networks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/v2/config/attr.html">Parameter Attribute</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/v2/data.html">数据访问</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/v2/run_logic.html">训练与应用</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/index_cn.html">FAQ</a></li>
</ul>

        
    </nav>
    
    <section class="doc-content-wrap">

      

 







<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
      
        <li><a href="../../index_cn.html">进阶指南</a> > </li>
      
        <li><a href="index_cn.html">RNN相关模型</a> > </li>
      
    <li>单双层RNN API对比介绍</li>
  </ul>
</div>
      
      <div class="wy-nav-content" id="doc-content">
        <div class="rst-content">
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="rnn-api">
<span id="algo-hrnn-rnn-api-compare"></span><h1>单双层RNN API对比介绍<a class="headerlink" href="#rnn-api" title="永久链接至标题">¶</a></h1>
<p>本文以PaddlePaddle的双层RNN单元测试为示例，用多对效果完全相同的、分别使用单双层RNN作为网络配置的模型，来讲解如何使用双层RNN。本文中所有的例子，都只是介绍双层RNN的API接口，并不是使用双层RNN解决实际的问题。如果想要了解双层RNN在具体问题中的使用，请参考<span class="xref std std-ref">algo_hrnn_demo</span>。本文中示例所使用的单元测试文件是<a class="reference external" href="https://github.com/reyoung/Paddle/blob/develop/paddle/gserver/tests/test_RecurrentGradientMachine.cpp">test_RecurrentGradientMachine.cpp</a>。</p>
<div class="section" id="rnn-memory">
<h2>示例1：双层RNN，子序列间无Memory<a class="headerlink" href="#rnn-memory" title="永久链接至标题">¶</a></h2>
<p>在双层RNN中的经典情况是将内层的每一个时间序列数据，分别进行序列操作；并且内层的序列操作之间独立无依赖，即不需要使用Memory。</p>
<p>在本示例中，单层RNN和双层RNN的网络配置，都是将每一句分好词后的句子，使用LSTM作为encoder，压缩成一个向量。区别是RNN使用两层序列模型，将多句话看成一个整体同时使用encoder压缩。二者语意上完全一致。这组语义相同的示例配置如下：</p>
<ul class="simple">
<li>单层RNN: <a class="reference external" href="https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/gserver/tests/sequence_layer_group.conf">sequence_layer_group.conf</a></li>
<li>双层RNN: <a class="reference external" href="https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/gserver/tests/sequence_nest_layer_group.conf">sequence_nest_layer_group.conf</a></li>
</ul>
<div class="section" id="id1">
<h3>读取双层序列数据<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h3>
<p>首先，本示例中使用的原始数据如下:</p>
<ul class="simple">
<li>本例中的原始数据一共有10个样本。每个样本由两部分组成，一个label（此处都为2）和一个已经分词后的句子。这个数据也被单层RNN网络直接使用。</li>
</ul>
<div class="highlight-text"><div class="highlight"><pre><span></span>2  	酒店 有 很 舒适 的 床垫 子 ， 床上用品 也 应该 是 一人 一 换 ， 感觉 很 利落 对 卫生 很 放心 呀 。
2  	很 温馨 ， 也 挺 干净 的 * 地段 不错 ， 出来 就 有 全家 ， 离 地铁站 也 近 ， 交通 很方便 * 就是 都 不 给 刷牙 的 杯子 啊 ， 就 第一天 给 了 一次性杯子 *
2  	位置 方便 ， 强烈推荐 ， 十一 出去玩 的 时候 选 的 ， 对面 就是 华润万家 ， 周围 吃饭 的 也 不少 。
2  	交通便利 ， 吃 很 便利 ， 乾 浄 、 安静 ， 商务 房 有 电脑 、 上网 快 ， 价格 可以 ， 就 早餐 不 好吃 。 整体 是 不错 的 。 適 合 出差 來 住 。
2  	本来 准备 住 两 晚 ， 第 2 天 一早 居然 停电 ， 且 无 通知 ， 只有 口头 道歉 。 总体来说 性价比 尚可 ， 房间 较 新 ， 还是 推荐 .
2  	这个 酒店 去过 很多 次 了 ， 选择 的 主要原因 是 离 客户 最 便宜 相对 又 近 的 酒店
2  	挺好 的 汉庭 ， 前台 服务 很 热情 ， 卫生 很 整洁 ， 房间 安静 ， 水温 适中 ， 挺好 ！
2  	HowardJohnson 的 品质 ， 服务 相当 好 的 一 家 五星级 。 房间 不错 、 泳池 不错 、 楼层 安排 很 合理 。 还有 就是 地理位置 ， 简直 一 流 。 就 在 天一阁 、 月湖 旁边 ， 离 天一广场 也 不远 。 下次 来 宁波 还会 住 。
2  	酒店 很干净 ， 很安静 ， 很 温馨 ， 服务员 服务 好 ， 各方面 都 不错 *
2  	挺好 的 ， 就是 没 窗户 ， 不过 对 得 起 这 价格
</pre></div>
</div>
<ul class="simple">
<li>双层序列数据一共有4个样本。 每个样本间用空行分开，整体数据和原始数据完全一样。但于双层序列的LSTM来说，第一个样本同时encode两条数据成两个向量。这四条数据同时处理的句子数量为<code class="code docutils literal"><span class="pre">[2,</span> <span class="pre">3,</span> <span class="pre">2,</span> <span class="pre">3]</span></code>。</li>
</ul>
<div class="highlight-text"><div class="highlight"><pre><span></span>2  	酒店 有 很 舒适 的 床垫 子 ， 床上用品 也 应该 是 一人 一 换 ， 感觉 很 利落 对 卫生 很 放心 呀 。
2  	很 温馨 ， 也 挺 干净 的 * 地段 不错 ， 出来 就 有 全家 ， 离 地铁站 也 近 ， 交通 很方便 * 就是 都 不 给 刷牙 的 杯子 啊 ， 就 第一天 给 了 一次性杯子 *

2  	位置 方便 ， 强烈推荐 ， 十一 出去玩 的 时候 选 的 ， 对面 就是 华润万家 ， 周围 吃饭 的 也 不少 。
2  	交通便利 ， 吃 很 便利 ， 乾 浄 、 安静 ， 商务 房 有 电脑 、 上网 快 ， 价格 可以 ， 就 早餐 不 好吃 。 整体 是 不错 的 。 適 合 出差 來 住 。
2  	本来 准备 住 两 晚 ， 第 2 天 一早 居然 停电 ， 且 无 通知 ， 只有 口头 道歉 。 总体来说 性价比 尚可 ， 房间 较 新 ， 还是 推荐 .

2  	这个 酒店 去过 很多 次 了 ， 选择 的 主要原因 是 离 客户 最 便宜 相对 又 近 的 酒店
2  	挺好 的 汉庭 ， 前台 服务 很 热情 ， 卫生 很 整洁 ， 房间 安静 ， 水温 适中 ， 挺好 ！

2  	HowardJohnson 的 品质 ， 服务 相当 好 的 一 家 五星级 。 房间 不错 、 泳池 不错 、 楼层 安排 很 合理 。 还有 就是 地理位置 ， 简直 一 流 。 就 在 天一阁 、 月湖 旁边 ， 离 天一广场 也 不远 。 下次 来 宁波 还会 住 。
2  	酒店 很干净 ， 很安静 ， 很 温馨 ， 服务员 服务 好 ， 各方面 都 不错 *
2  	挺好 的 ， 就是 没 窗户 ， 不过 对 得 起 这 价格
</pre></div>
</div>
<p>其次，对于两种不同的输入数据类型，不同DataProvider对比如下(<a class="reference external" href="https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/gserver/tests/sequenceGen.py">sequenceGen.py</a>)：</p>
<div class="highlight-python"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">hook</span><span class="p">(</span><span class="n">settings</span><span class="p">,</span> <span class="n">dict_file</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">settings</span><span class="o">.</span><span class="n">word_dict</span> <span class="o">=</span> <span class="n">dict_file</span>
    <span class="n">settings</span><span class="o">.</span><span class="n">input_types</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">integer_value_sequence</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">settings</span><span class="o">.</span><span class="n">word_dict</span><span class="p">)),</span> <span class="n">integer_value</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="n">settings</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;dict len : </span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">settings</span><span class="o">.</span><span class="n">word_dict</span><span class="p">)))</span>


<span class="nd">@provider</span><span class="p">(</span><span class="n">init_hook</span><span class="o">=</span><span class="n">hook</span><span class="p">,</span> <span class="n">should_shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">process</span><span class="p">(</span><span class="n">settings</span><span class="p">,</span> <span class="n">file_name</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file_name</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fdata</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">fdata</span><span class="p">:</span>
            <span class="n">label</span><span class="p">,</span> <span class="n">comment</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="n">label</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">label</span><span class="o">.</span><span class="n">split</span><span class="p">()))</span>
            <span class="n">words</span> <span class="o">=</span> <span class="n">comment</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
            <span class="n">words</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">settings</span><span class="o">.</span><span class="n">word_dict</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">settings</span><span class="o">.</span><span class="n">word_dict</span>
            <span class="p">]</span>
            <span class="k">yield</span> <span class="n">words</span><span class="p">,</span> <span class="n">label</span>
</pre></div>
</td></tr></table></div>
<ul class="simple">
<li>这是普通的单层时间序列的DataProvider代码，其说明如下：<ul>
<li>DataProvider共返回两个数据，分别是words和label。即上述代码中的第19行。<ul>
<li>words是原始数据中的每一句话，所对应的词表index数组。它是integer_value_sequence类型的，即整数数组。words即为这个数据中的单层时间序列。</li>
<li>label是原始数据中对于每一句话的分类标签，它是integer_value类型的。</li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="highlight-python"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="c1">## for hierarchical sequence network</span>
<span class="k">def</span> <span class="nf">hook2</span><span class="p">(</span><span class="n">settings</span><span class="p">,</span> <span class="n">dict_file</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">settings</span><span class="o">.</span><span class="n">word_dict</span> <span class="o">=</span> <span class="n">dict_file</span>
    <span class="n">settings</span><span class="o">.</span><span class="n">input_types</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">integer_value_sub_sequence</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">settings</span><span class="o">.</span><span class="n">word_dict</span><span class="p">)),</span>
        <span class="n">integer_value_sequence</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="n">settings</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;dict len : </span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">settings</span><span class="o">.</span><span class="n">word_dict</span><span class="p">)))</span>


<span class="nd">@provider</span><span class="p">(</span><span class="n">init_hook</span><span class="o">=</span><span class="n">hook2</span><span class="p">,</span> <span class="n">should_shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">process2</span><span class="p">(</span><span class="n">settings</span><span class="p">,</span> <span class="n">file_name</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file_name</span><span class="p">)</span> <span class="k">as</span> <span class="n">fdata</span><span class="p">:</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">sentences</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">fdata</span><span class="p">:</span>
            <span class="k">if</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">line</span><span class="p">))</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">label</span><span class="p">,</span> <span class="n">comment</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">)</span>
                <span class="n">label</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">label</span><span class="o">.</span><span class="n">split</span><span class="p">()))</span>
                <span class="n">words</span> <span class="o">=</span> <span class="n">comment</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
                <span class="n">words</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="n">settings</span><span class="o">.</span><span class="n">word_dict</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span>
                    <span class="k">if</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">settings</span><span class="o">.</span><span class="n">word_dict</span>
                <span class="p">]</span>
                <span class="n">labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>
                <span class="n">sentences</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">yield</span> <span class="n">sentences</span><span class="p">,</span> <span class="n">labels</span>
                <span class="n">labels</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="n">sentences</span> <span class="o">=</span> <span class="p">[]</span>
</pre></div>
</td></tr></table></div>
<ul class="simple">
<li>对于同样的数据，双层时间序列的DataProvider的代码。其说明如下：<ul>
<li>DataProvider共返回两组数据，分别是sentences和labels。即在双层序列的原始数据中，每一组内的所有句子和labels</li>
<li>sentences是双层时间序列的数据。由于它内部包含了每组数据中的所有句子，且每个句子表示为对应的词表索引数组，因此它是integer_value_sub_sequence 类型的，即双层时间序列。</li>
<li>labels是每组内每个句子的标签，故而是一个单层时间序列。</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="id2">
<h3>模型配置的模型配置<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h3>
<p>首先，我们看一下单层RNN的配置。代码中9-15行(高亮部分)即为单层RNN序列的使用代码。这里使用了PaddlePaddle预定义好的RNN处理函数。在这个函数中，RNN对于每一个时间步通过了一个LSTM网络。</p>
<div class="highlight-python"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">data_layer</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;word&quot;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">dict_dim</span><span class="p">)</span>

<span class="n">emb</span> <span class="o">=</span> <span class="n">embedding_layer</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">word_dim</span><span class="p">)</span>

<span class="c1"># (lstm_input + lstm) is equal to lstmemory </span>
<span class="k">with</span> <span class="n">mixed_layer</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)</span> <span class="k">as</span> <span class="n">lstm_input</span><span class="p">:</span>
    <span class="n">lstm_input</span> <span class="o">+=</span> <span class="n">full_matrix_projection</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">emb</span><span class="p">)</span>

<span class="hll"><span class="n">lstm</span> <span class="o">=</span> <span class="n">lstmemory_group</span><span class="p">(</span>
</span><span class="hll">    <span class="nb">input</span><span class="o">=</span><span class="n">lstm_input</span><span class="p">,</span>
</span><span class="hll">    <span class="n">size</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">,</span>
</span><span class="hll">    <span class="n">act</span><span class="o">=</span><span class="n">TanhActivation</span><span class="p">(),</span>
</span><span class="hll">    <span class="n">gate_act</span><span class="o">=</span><span class="n">SigmoidActivation</span><span class="p">(),</span>
</span><span class="hll">    <span class="n">state_act</span><span class="o">=</span><span class="n">TanhActivation</span><span class="p">())</span>
</span><span class="hll">
</span><span class="n">lstm_last</span> <span class="o">=</span> <span class="n">last_seq</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">lstm</span><span class="p">)</span>

<span class="k">with</span> <span class="n">mixed_layer</span><span class="p">(</span>
        <span class="n">size</span><span class="o">=</span><span class="n">label_dim</span><span class="p">,</span> <span class="n">act</span><span class="o">=</span><span class="n">SoftmaxActivation</span><span class="p">(),</span> <span class="n">bias_attr</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">output</span><span class="p">:</span>
    <span class="n">output</span> <span class="o">+=</span> <span class="n">full_matrix_projection</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">lstm_last</span><span class="p">)</span>

<span class="n">outputs</span><span class="p">(</span>
    <span class="n">classification_cost</span><span class="p">(</span>
        <span class="nb">input</span><span class="o">=</span><span class="n">output</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">data_layer</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;label&quot;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">)))</span>
</pre></div>
</td></tr></table></div>
<p>其次，我们看一下语义相同的双层RNN的网络配置:</p>
<ul class="simple">
<li>PaddlePaddle中的许多layer并不在意输入是否是时间序列，例如<code class="code docutils literal"><span class="pre">embedding_layer</span></code>。在这些layer中，所有的操作都是针对每一个时间步来进行的。</li>
<li>在该配置的7-26行(高亮部分)，将双层时间序列数据先变换成单层时间序列数据，再对每一个单层时间序列进行处理。<ul>
<li>使用<code class="code docutils literal"><span class="pre">recurrent_group</span></code>这个函数进行变换，在变换时需要将输入序列传入。由于我们想要的变换是双层时间序列=&gt; 单层时间序列，所以我们需要将输入数据标记成<code class="code docutils literal"><span class="pre">SubsequenceInput</span></code>。</li>
<li>在本例中，我们将原始数据的每一组，通过<code class="code docutils literal"><span class="pre">recurrent_group</span></code>进行拆解，拆解成的每一句话再通过一个LSTM网络。这和单层RNN的配置是等价的。</li>
</ul>
</li>
<li>与单层RNN的配置类似，我们只需要使用LSTM encode成的最后一个向量。所以对<code class="code docutils literal"><span class="pre">recurrent_group</span></code>进行了<code class="code docutils literal"><span class="pre">last_seq</span></code>操作。但和单层RNN不同，我们是对每一个子序列取最后一个元素，因此<code class="code docutils literal"><span class="pre">agg_level=AggregateLevel.EACH_SEQUENCE</span></code>。</li>
<li>至此，<code class="code docutils literal"><span class="pre">lstm_last</span></code>便和单层RNN配置中的<code class="code docutils literal"><span class="pre">lstm_last</span></code>具有相同的结果了。</li>
</ul>
<div class="highlight-python"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">data_layer</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;word&quot;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">dict_dim</span><span class="p">)</span>

<span class="n">emb_group</span> <span class="o">=</span> <span class="n">embedding_layer</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">word_dim</span><span class="p">)</span>


<span class="c1"># (lstm_input + lstm) is equal to lstmemory </span>
<span class="hll"><span class="k">def</span> <span class="nf">lstm_group</span><span class="p">(</span><span class="n">lstm_group_input</span><span class="p">):</span>
</span><span class="hll">    <span class="k">with</span> <span class="n">mixed_layer</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)</span> <span class="k">as</span> <span class="n">group_input</span><span class="p">:</span>
</span><span class="hll">        <span class="n">group_input</span> <span class="o">+=</span> <span class="n">full_matrix_projection</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">lstm_group_input</span><span class="p">)</span>
</span><span class="hll">
</span><span class="hll">    <span class="n">lstm_output</span> <span class="o">=</span> <span class="n">lstmemory_group</span><span class="p">(</span>
</span><span class="hll">        <span class="nb">input</span><span class="o">=</span><span class="n">group_input</span><span class="p">,</span>
</span><span class="hll">        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;lstm_group&quot;</span><span class="p">,</span>
</span><span class="hll">        <span class="n">size</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">,</span>
</span><span class="hll">        <span class="n">act</span><span class="o">=</span><span class="n">TanhActivation</span><span class="p">(),</span>
</span><span class="hll">        <span class="n">gate_act</span><span class="o">=</span><span class="n">SigmoidActivation</span><span class="p">(),</span>
</span><span class="hll">        <span class="n">state_act</span><span class="o">=</span><span class="n">TanhActivation</span><span class="p">())</span>
</span><span class="hll">    <span class="k">return</span> <span class="n">lstm_output</span>
</span><span class="hll">
</span><span class="hll">
</span><span class="hll"><span class="n">lstm_nest_group</span> <span class="o">=</span> <span class="n">recurrent_group</span><span class="p">(</span>
</span><span class="hll">    <span class="nb">input</span><span class="o">=</span><span class="n">SubsequenceInput</span><span class="p">(</span><span class="n">emb_group</span><span class="p">),</span> <span class="n">step</span><span class="o">=</span><span class="n">lstm_group</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;lstm_nest_group&quot;</span><span class="p">)</span>
</span><span class="hll"><span class="c1"># hasSubseq -&gt;(seqlastins) seq</span>
</span><span class="hll"><span class="n">lstm_last</span> <span class="o">=</span> <span class="n">last_seq</span><span class="p">(</span>
</span><span class="hll">    <span class="nb">input</span><span class="o">=</span><span class="n">lstm_nest_group</span><span class="p">,</span> <span class="n">agg_level</span><span class="o">=</span><span class="n">AggregateLevel</span><span class="o">.</span><span class="n">EACH_SEQUENCE</span><span class="p">)</span>
</span><span class="hll">
</span><span class="c1"># seq -&gt;(expand) hasSubseq</span>
</pre></div>
</td></tr></table></div>
</div>
</div>
<div class="section" id="id3">
<h2>示例2：双层RNN，子序列间有Memory<a class="headerlink" href="#id3" title="永久链接至标题">¶</a></h2>
<p>本示例意图使用单层RNN和双层RNN实现两个完全等价的全连接RNN。</p>
<ul class="simple">
<li>对于单层RNN，输入数据为一个完整的时间序列，例如<code class="code docutils literal"><span class="pre">[4,</span> <span class="pre">5,</span> <span class="pre">2,</span> <span class="pre">0,</span> <span class="pre">9,</span> <span class="pre">8,</span> <span class="pre">1,</span> <span class="pre">4]</span></code>。</li>
<li>对于双层RNN，输入数据为在单层RNN数据里面，任意将一些数据组合成双层时间序列，例如<code class="code docutils literal"><span class="pre">[</span> <span class="pre">[4,</span> <span class="pre">5,</span> <span class="pre">2],</span> <span class="pre">[0,</span> <span class="pre">9],</span> <span class="pre">[8,</span> <span class="pre">1,</span> <span class="pre">4]]</span></code>。</li>
</ul>
<div class="section" id="id4">
<h3>模型配置的模型配置<a class="headerlink" href="#id4" title="永久链接至标题">¶</a></h3>
<p>我们选取单双层序列配置中的不同部分，来对比分析两者语义相同的原因。</p>
<ul class="simple">
<li>单层RNN：过了一个很简单的recurrent_group。每一个时间步，当前的输入y和上一个时间步的输出rnn_state做了一个全链接。</li>
</ul>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
    <span class="n">mem</span> <span class="o">=</span> <span class="n">memory</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;rnn_state&quot;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">fc_layer</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="p">[</span><span class="n">y</span><span class="p">,</span> <span class="n">mem</span><span class="p">],</span>
                    <span class="n">size</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">,</span>
                    <span class="n">act</span><span class="o">=</span><span class="n">TanhActivation</span><span class="p">(),</span>
                    <span class="n">bias_attr</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;rnn_state&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>

<span class="n">out</span> <span class="o">=</span> <span class="n">recurrent_group</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;rnn&quot;</span><span class="p">,</span>
    <span class="n">step</span><span class="o">=</span><span class="n">step</span><span class="p">,</span>
    <span class="nb">input</span><span class="o">=</span><span class="n">emb</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li>双层RNN，外层memory是一个元素：<ul>
<li>内层inner_step的recurrent_group和单层序列的几乎一样。除了boot_layer=outer_mem，表示将外层的outer_mem作为内层memory的初始状态。外层outer_step中，outer_mem是一个子句的最后一个向量，即整个双层group是将前一个子句的最后一个向量，作为下一个子句memory的初始状态。</li>
<li>从输入数据上看，单双层序列的句子是一样的，只是双层序列将其又做了子序列划分。因此双层序列的配置中，必须将前一个子句的最后一个元素，作为boot_layer传给下一个子句的memory，才能保证和单层序列的配置中“每个时间步都用了上一个时间步的输出结果”一致。</li>
</ul>
</li>
</ul>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">outer_step</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">outer_mem</span> <span class="o">=</span> <span class="n">memory</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;outer_rnn_state&quot;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">inner_step</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
        <span class="n">inner_mem</span> <span class="o">=</span> <span class="n">memory</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;inner_rnn_state&quot;</span><span class="p">,</span>
                           <span class="n">size</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">,</span>
                           <span class="n">boot_layer</span><span class="o">=</span><span class="n">outer_mem</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">fc_layer</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="p">[</span><span class="n">y</span><span class="p">,</span> <span class="n">inner_mem</span><span class="p">],</span>
                        <span class="n">size</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">,</span>
                        <span class="n">act</span><span class="o">=</span><span class="n">TanhActivation</span><span class="p">(),</span>
                        <span class="n">bias_attr</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;inner_rnn_state&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="n">inner_rnn_output</span> <span class="o">=</span> <span class="n">recurrent_group</span><span class="p">(</span>
        <span class="n">step</span><span class="o">=</span><span class="n">inner_step</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;inner&quot;</span><span class="p">,</span>
        <span class="nb">input</span><span class="o">=</span><span class="n">x</span><span class="p">)</span>
    <span class="n">last</span> <span class="o">=</span> <span class="n">last_seq</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">inner_rnn_output</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;outer_rnn_state&quot;</span><span class="p">)</span>

    <span class="c1"># &quot;return last&quot; won&#39;t work, because recurrent_group only support the input </span>
    <span class="c1"># sequence type is same as return sequence type.</span>
    <span class="k">return</span> <span class="n">inner_rnn_output</span>

<span class="n">out</span> <span class="o">=</span> <span class="n">recurrent_group</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;outer&quot;</span><span class="p">,</span>
    <span class="n">step</span><span class="o">=</span><span class="n">outer_step</span><span class="p">,</span>
    <span class="nb">input</span><span class="o">=</span><span class="n">SubsequenceInput</span><span class="p">(</span><span class="n">emb</span><span class="p">))</span>

</pre></div>
</div>
<div class="admonition warning">
<p class="first admonition-title">警告</p>
<p class="last">PaddlePaddle目前只支持在每个时间步中，Memory的时间序列长度一致的情况。</p>
</div>
</div>
</div>
<div class="section" id="rnn">
<h2>示例3：双层RNN，输入不等长<a class="headerlink" href="#rnn" title="永久链接至标题">¶</a></h2>
<style> .red {color:red} </style><p><strong>输入不等长</strong> 是指recurrent_group的多个输入序列，在每个时间步的子序列长度可以不相等。但序列输出时，需要指定与某一个输入的序列信息是一致的。使用<span class="red">targetInlink</span>可以指定哪一个输入和输出序列信息一致，默认指定第一个输入。</p>
<p>示例3的配置分别为<a class="reference external" href="https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/gserver/tests/sequence_rnn_multi_unequalength_inputs.conf">单层不等长RNN</a>和<a class="reference external" href="https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/gserver/tests/sequence_nest_rnn_multi_unequalength_inputs.conf">双层不等长RNN</a>。</p>
<p>示例3对于单层RNN和双层RNN数据完全相同。</p>
<ul class="simple">
<li>对于单层RNN的数据一共有两个样本，他们分别是<code class="code docutils literal"><span class="pre">[1,</span> <span class="pre">2,</span> <span class="pre">4,</span> <span class="pre">5,</span> <span class="pre">2],</span> <span class="pre">[5,</span> <span class="pre">4,</span> <span class="pre">1,</span> <span class="pre">3,</span> <span class="pre">1]</span></code>和<code class="code docutils literal"><span class="pre">[0,</span> <span class="pre">2,</span> <span class="pre">2,</span> <span class="pre">5,</span> <span class="pre">0,</span> <span class="pre">1,</span> <span class="pre">2],</span> <span class="pre">[1,</span> <span class="pre">5,</span> <span class="pre">4,</span> <span class="pre">2,</span> <span class="pre">3,</span> <span class="pre">6,</span> <span class="pre">1]</span></code>。对于每一个单层RNN的数据，均有两组特征。</li>
<li>在单层数据的基础上，双层RNN数据随意加了一些隔断，例如将第一条数据转化为<code class="code docutils literal"><span class="pre">[[0,</span> <span class="pre">2],</span> <span class="pre">[2,</span> <span class="pre">5],</span> <span class="pre">[0,</span> <span class="pre">1,</span> <span class="pre">2]],[[1,</span> <span class="pre">5],</span> <span class="pre">[4],</span> <span class="pre">[2,</span> <span class="pre">3,</span> <span class="pre">6,</span> <span class="pre">1]]</span></code>。</li>
<li>需要注意的是PaddlePaddle目前只支持子序列数目一样的多输入双层RNN。例如本例中的两个特征，均有三个子序列。每个子序列长度可以不一致，但是子序列的数目必须一样。</li>
</ul>
<div class="section" id="id7">
<h3>模型配置<a class="headerlink" href="#id7" title="永久链接至标题">¶</a></h3>
<p>和示例2中的配置类似，示例3的配置使用了单层RNN和双层RNN，实现两个完全等价的全连接RNN。</p>
<ul class="simple">
<li>单层RNN:</li>
</ul>
<div class="highlight-python"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">calrnn</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
        <span class="n">mem</span> <span class="o">=</span> <span class="n">memory</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;rnn_state_&#39;</span> <span class="o">+</span> <span class="n">y</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">fc_layer</span><span class="p">(</span>
            <span class="nb">input</span><span class="o">=</span><span class="p">[</span><span class="n">y</span><span class="p">,</span> <span class="n">mem</span><span class="p">],</span>
            <span class="n">size</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">,</span>
            <span class="n">act</span><span class="o">=</span><span class="n">TanhActivation</span><span class="p">(),</span>
            <span class="n">bias_attr</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s1">&#39;rnn_state_&#39;</span> <span class="o">+</span> <span class="n">y</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="n">encoder1</span> <span class="o">=</span> <span class="n">calrnn</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
    <span class="n">encoder2</span> <span class="o">=</span> <span class="n">calrnn</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">encoder1</span><span class="p">,</span> <span class="n">encoder2</span><span class="p">]</span>


<span class="n">encoder1_rep</span><span class="p">,</span> <span class="n">encoder2_rep</span> <span class="o">=</span> <span class="n">recurrent_group</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;stepout&quot;</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="n">step</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="p">[</span><span class="n">emb1</span><span class="p">,</span> <span class="n">emb2</span><span class="p">])</span>
</pre></div>
</td></tr></table></div>
<ul class="simple">
<li>双层RNN:</li>
</ul>
<div class="highlight-python"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">outer_step</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
    <span class="n">index</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">inner_step</span><span class="p">(</span><span class="n">ipt</span><span class="p">):</span>
        <span class="n">index</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">index</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">outer_mem</span> <span class="o">=</span> <span class="n">memory</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;outer_rnn_state_</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">i</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">inner_step_impl</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
            <span class="n">inner_mem</span> <span class="o">=</span> <span class="n">memory</span><span class="p">(</span>
                <span class="n">name</span><span class="o">=</span><span class="s2">&quot;inner_rnn_state_&quot;</span> <span class="o">+</span> <span class="n">y</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                <span class="n">size</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">,</span>
                <span class="n">boot_layer</span><span class="o">=</span><span class="n">outer_mem</span><span class="p">)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">fc_layer</span><span class="p">(</span>
                <span class="nb">input</span><span class="o">=</span><span class="p">[</span><span class="n">y</span><span class="p">,</span> <span class="n">inner_mem</span><span class="p">],</span>
                <span class="n">size</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">,</span>
                <span class="n">act</span><span class="o">=</span><span class="n">TanhActivation</span><span class="p">(),</span>
                <span class="n">bias_attr</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;inner_rnn_state_&#39;</span> <span class="o">+</span> <span class="n">y</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">out</span>

        <span class="n">encoder</span> <span class="o">=</span> <span class="n">recurrent_group</span><span class="p">(</span>
            <span class="n">step</span><span class="o">=</span><span class="n">inner_step_impl</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;inner_</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="n">ipt</span><span class="p">)</span>
        <span class="n">last</span> <span class="o">=</span> <span class="n">last_seq</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;outer_rnn_state_</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">i</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="n">encoder</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">last</span>

    <span class="n">encoder1</span><span class="p">,</span> <span class="n">sentence_last_state1</span> <span class="o">=</span> <span class="n">inner_step</span><span class="p">(</span><span class="n">ipt</span><span class="o">=</span><span class="n">x1</span><span class="p">)</span>
    <span class="n">encoder2</span><span class="p">,</span> <span class="n">sentence_last_state2</span> <span class="o">=</span> <span class="n">inner_step</span><span class="p">(</span><span class="n">ipt</span><span class="o">=</span><span class="n">x2</span><span class="p">)</span>

    <span class="n">encoder1_expand</span> <span class="o">=</span> <span class="n">expand_layer</span><span class="p">(</span>
        <span class="nb">input</span><span class="o">=</span><span class="n">sentence_last_state1</span><span class="p">,</span> <span class="n">expand_as</span><span class="o">=</span><span class="n">encoder2</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">[</span><span class="n">encoder1_expand</span><span class="p">,</span> <span class="n">encoder2</span><span class="p">]</span>


<span class="n">encoder1_rep</span><span class="p">,</span> <span class="n">encoder2_rep</span> <span class="o">=</span> <span class="n">recurrent_group</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;outer&quot;</span><span class="p">,</span>
    <span class="n">step</span><span class="o">=</span><span class="n">outer_step</span><span class="p">,</span>
    <span class="nb">input</span><span class="o">=</span><span class="p">[</span><span class="n">SubsequenceInput</span><span class="p">(</span><span class="n">emb1</span><span class="p">),</span> <span class="n">SubsequenceInput</span><span class="p">(</span><span class="n">emb2</span><span class="p">)],</span>
    <span class="n">targetInlink</span><span class="o">=</span><span class="n">emb2</span><span class="p">)</span>
</pre></div>
</td></tr></table></div>
<p>在上面代码中，单层和双层序列的使用和示例2中的示例类似，区别是同时处理了两个输入。而对于双层序列，两个输入的子序列长度也并不相同。但是，我们使用了<code class="code docutils literal"><span class="pre">targetInlink</span></code>参数设置了外层<code class="code docutils literal"><span class="pre">recurrent_group</span></code>的输出格式。所以外层输出的序列形状，和<code class="code docutils literal"><span class="pre">emb2</span></code>的序列形状一致。</p>
</div>
</div>
<div class="section" id="beam-search">
<h2>示例4：beam_search的生成<a class="headerlink" href="#beam-search" title="永久链接至标题">¶</a></h2>
<p>TBD</p>
</div>
<div class="section" id="id8">
<h2>词汇表<a class="headerlink" href="#id8" title="永久链接至标题">¶</a></h2>
<div class="section" id="memory">
<span id="glossary-memory"></span><h3>Memory<a class="headerlink" href="#memory" title="永久链接至标题">¶</a></h3>
<p>Memory是PaddlePaddle实现RNN时候使用的一个概念。RNN即时间递归神经网络，通常要求时间步之间具有一些依赖性，即当前时间步下的神经网络依赖前一个时间步神经网络中某一个神经元输出。如下图所示。</p>
<img src="../../../_images/graphviz-def0e483abd6c62214b13e553c3727677a6f0a99.png" alt="digraph G{
	subgraph cluster_timestep0 {
		label=&quot;recurrent timestep i-1&quot;
		bgcolor=lightgray
		node [style=filled,color=white]
		fc0_0 [label=&quot;fc 0&quot;]
		fc0_1 [label=&quot;fc 1&quot;]
		fc0_2 [label=&quot;fc 2&quot;]

		fc0_0 -&gt; fc0_1
		fc0_1 -&gt; fc0_2
	}

	subgraph cluster_timestep1 {
		label=&quot;recurrent timestep i&quot;
		node [style=filled];
		fc1_0 [label=&quot;fc 0&quot;]
		fc1_1 [label=&quot;fc 1&quot;]
		fc1_2 [label=&quot;fc 2&quot;]
		color=blue

		fc1_0 -&gt; fc1_1
		fc1_1 -&gt; fc1_2
	}

	subgraph cluster_timestep2 {
		label=&quot;recurrent timestep i+1&quot;
		bgcolor=lightgray
		node [style=filled,color=white]
		fc2_0 [label=&quot;fc 0&quot;]
		fc2_1 [label=&quot;fc 1&quot;]
		fc2_2 [label=&quot;fc 2&quot;]

		fc2_0 -&gt; fc2_1
		fc2_1 -&gt; fc2_2
	}
	
	
	fc0_1 -&gt; fc1_1 [style=&quot;dotted&quot; constraint=false]
	fc1_1 -&gt; fc2_1 [style=&quot;dotted&quot; constraint=false]

}" />
<p>上图中虚线的连接，即是跨越时间步的网络连接。PaddlePaddle在实现RNN的时候，将这种跨越时间步的连接用一个特殊的神经网络单元实现。这个神经网络单元就叫Memory。Memory可以缓存上一个时刻某一个神经元的输出，然后在下一个时间步输入给另一个神经元。使用Memory的RNN实现便如下图所示。</p>
<img src="../../../_images/graphviz-7ca2be3fe111ea3f93b646fae7945ba707984fd4.png" alt="digraph G{
	subgraph cluster_timestep0 {
		label=&quot;recurrent timestep i-1&quot;
		bgcolor=lightgray
		node [style=filled,color=white]
		fc0_0 [label=&quot;fc 0&quot;]
		fc0_1 [label=&quot;fc 1&quot;]
		fc0_2 [label=&quot;fc 2&quot;]
		m0 [label=&quot;memory&quot;]
		fc0_0 -&gt; fc0_1
		fc0_1 -&gt; fc0_2
		fc0_1 -&gt; m0
		m0 -&gt; fc0_1
	}

	subgraph cluster_timestep1 {
		label=&quot;recurrent timestep i&quot;
		node [style=filled];
		fc1_0 [label=&quot;fc 0&quot;]
		fc1_1 [label=&quot;fc 1&quot;]
		fc1_2 [label=&quot;fc 2&quot;]
		m1 [label=&quot;memory&quot;]
		color=blue
		fc1_0 -&gt; fc1_1
		fc1_1 -&gt; fc1_2
		fc1_1 -&gt; m1
		m1 -&gt; fc1_1
	}

	subgraph cluster_timestep2 {
		label=&quot;recurrent timestep i+1&quot;
		bgcolor=lightgray
		node [style=filled,color=white]
		fc2_0 [label=&quot;fc 0&quot;]
		fc2_1 [label=&quot;fc 1&quot;]
		fc2_2 [label=&quot;fc 2&quot;]
		m2 [label=&quot;memory&quot;]
		fc2_0 -&gt; fc2_1
		fc2_1 -&gt; fc2_2
		fc2_1 -&gt; m2
		m2 -&gt; fc2_1
	}
	
	
	m0 -&gt; m1 [style=&quot;dotted&quot; constraint=false]
	m1 -&gt; m2 [style=&quot;dotted&quot; constraint=false]

}" />
<p>使用这种方式，PaddlePaddle可以比较简单的判断哪些输出是应该跨越时间步的，哪些不是。</p>
</div>
<div class="section" id="glossary-timestep">
<span id="id9"></span><h3>时间步<a class="headerlink" href="#glossary-timestep" title="永久链接至标题">¶</a></h3>
<p>参考时间序列。</p>
</div>
<div class="section" id="glossary-sequence">
<span id="id10"></span><h3>时间序列<a class="headerlink" href="#glossary-sequence" title="永久链接至标题">¶</a></h3>
<p>时间序列(time series)是指一系列的特征数据。这些特征数据之间的顺序是有意义的。即特征的数组，而不是特征的集合。而这每一个数组元素，或者每一个系列里的特征数据，即为一个时间步(time step)。值得注意的是，时间序列、时间步的概念，并不真正的和『时间』有关。只要一系列特征数据中的『顺序』是有意义的，即为时间序列的输入。</p>
<p>举例说明，例如文本分类中，我们通常将一句话理解成一个时间序列。比如一句话中的每一个单词，会变成词表中的位置。而这一句话就可以表示成这些位置的数组。例如 <code class="code docutils literal"><span class="pre">[9,</span> <span class="pre">2,</span> <span class="pre">3,</span> <span class="pre">5,</span> <span class="pre">3]</span></code> 。</p>
<p>关于时间序列(time series)的更详细准确的定义，可以参考 <a class="reference external" href="https://en.wikipedia.org/wiki/Time_series">维基百科页面 Time series</a> 或者 <a class="reference external" href="https://zh.wikipedia.org/wiki/%E6%99%82%E9%96%93%E5%BA%8F%E5%88%97">维基百科中文页面 时间序列</a> 。</p>
<p>另外，Paddle中经常会将时间序列成为 <code class="code docutils literal"><span class="pre">Sequence</span></code> 。他们在Paddle的文档和API中是一个概念。</p>
</div>
<div class="section" id="glossary-rnn">
<span id="id12"></span><h3>RNN<a class="headerlink" href="#glossary-rnn" title="永久链接至标题">¶</a></h3>
<p>RNN 在PaddlePaddle的文档中，一般表示 <code class="code docutils literal"><span class="pre">Recurrent</span> <span class="pre">neural</span> <span class="pre">network</span></code>，即时间递归神经网络。详细介绍可以参考 <a class="reference external" href="https://en.wikipedia.org/wiki/Recurrent_neural_network">维基百科页面 Recurrent neural network</a> 或者 <a class="reference external" href="https://zh.wikipedia.org/wiki/%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">中文维基百科页面</a> 中关于时间递归神经网络的介绍。</p>
<p>RNN 一般在PaddlePaddle中，指对于一个时间序列输入数据，每一个时间步之间的神经网络具有一定的相关性。例如，某一个神经元的一个输入为上一个时间步网络中某一个神经元的输出。或者，从每一个时间步来看，神经网络的网络结构中具有有向环结构。</p>
</div>
<div class="section" id="id14">
<span id="id15"></span><h3>双层RNN<a class="headerlink" href="#id14" title="永久链接至标题">¶</a></h3>
<p>双层RNN顾名思义，即RNN之间有一次嵌套关系。输入数据整体上是一个时间序列，而对于每一个内层特征数据而言，也是一个时间序列。即二维数组，或者数组的数组这个概念。 而双层RNN是可以处理这种输入数据的网络结构。</p>
<p>例如，对于段落的文本分类，即将一段话进行分类。我们将一段话看成句子的数组，每个句子又是单词的数组。这便是一种双层RNN的输入数据。而将这个段落的每一句话用lstm编码成一个向量，再对每一句话的编码向量用lstm编码成一个段落的向量。再对这个段落向量进行分类，即为这个双层RNN的网络结构。</p>
</div>
</div>
</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../../optimization/gpu_profiling_cn.html" class="btn btn-neutral float-right" title="GPU性能分析与调优" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="hierarchical_layer_cn.html" class="btn btn-neutral" title="支持双层序列作为输入的Layer" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2016, PaddlePaddle developers.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../../',
            VERSION:'',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: ".txt",
        };
    </script>
      <script type="text/javascript" src="../../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../../_static/doctools.js"></script>
      <script type="text/javascript" src="../../../_static/translations.js"></script>
      <script type="text/javascript" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js"></script>
       
  

  
  
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>
  
  
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/perfect-scrollbar/0.6.14/js/perfect-scrollbar.jquery.min.js"></script>
  <script src="../../../_static/js/paddle_doc_init.js"></script> 

</body>
</html>